{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Pra-pemrosesan Data**"
      ],
      "metadata": {
        "id": "nk9zqJUKXLCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install library yang dibutuhkan\n",
        "!pip install sastrawi\n",
        "!pip install nltk\n",
        "\n",
        "# Import library yang dibutuhkan\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from google.colab import files\n",
        "\n",
        "# Download data tokenizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Setup Stemmer dan Stopword Remover\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "stopword_factory = StopWordRemoverFactory()\n",
        "stopwords = stopword_factory.get_stop_words()\n",
        "\n",
        "# Load Dataset tweet\n",
        "tweet_filename = 'crawling-data-merge.csv'\n",
        "df = pd.read_csv(tweet_filename)\n",
        "\n",
        "# Hitung jumlah data awal\n",
        "initial_data_count = len(df)\n",
        "print(f\"Jumlah data awal: {initial_data_count:,}\")\n",
        "\n",
        "# Load kamus slang yang sudah didownload dalam bentuk .csv (repo: insomniagung/kamus_kbba)\n",
        "alay_df = pd.read_csv('kamus_slang.csv')\n",
        "alay_df['slang'] = alay_df['slang'].astype(str).str.lower()\n",
        "alay_df['formal'] = alay_df['formal'].astype(str).str.lower()\n",
        "alay_dict = dict(zip(alay_df['slang'], alay_df['formal']))\n",
        "\n",
        "# Filter Tweet Berdasarkan Kata Kunci\n",
        "# Kata kunci yang wajib ada dan yang harus dihindari\n",
        "keyword_includes = ['coretax']     # WAJIB ADA dalam teks\n",
        "keyword_excludes = []  # TIDAK BOLEH ADA dalam teks\n",
        "\n",
        "def should_keep(text):\n",
        "    if pd.isnull(text):\n",
        "        return False\n",
        "    text = text.lower()\n",
        "\n",
        "    # Harus mengandung setidaknya satu kata dari keyword_includes\n",
        "    contains_include = any(re.search(rf'\\b{re.escape(word)}\\b', text) for word in keyword_includes)\n",
        "\n",
        "    # Tidak boleh mengandung satupun dari keyword_excludes\n",
        "    contains_exclude = any(re.search(rf'\\b{re.escape(word)}\\b', text) for word in keyword_excludes)\n",
        "\n",
        "    return contains_include and not contains_exclude\n",
        "\n",
        "# Filter berdasarkan kedua kondisi\n",
        "df = df[df['full_text'].apply(should_keep)]\n",
        "\n",
        "# Hitung jumlah data setelah filtering\n",
        "after_filter_count = len(df)\n",
        "print(f\"Jumlah data setelah filtering: {after_filter_count:,}\")\n",
        "print(f\"Data yang dibuang karena filtering: {initial_data_count - after_filter_count:,}\")\n",
        "\n",
        "# 2. Hapus duplikat\n",
        "df = df.drop_duplicates(subset='full_text').reset_index(drop=True)\n",
        "\n",
        "# Hitung jumlah data setelah menghapus duplikat\n",
        "after_dedup_count = len(df)\n",
        "print(f\"Jumlah data setelah menghapus duplikat: {after_dedup_count:,}\")\n",
        "print(f\"Data duplikat yang dihapus: {after_filter_count - after_dedup_count:,}\")\n",
        "\n",
        "# 3. Normalisasi kata slang\n",
        "def normalize_text(text):\n",
        "    return ' '.join([alay_dict.get(word, word) for word in text.split()])\n",
        "\n",
        "# Handle elongation pada data\n",
        "def remove_elongation(text):\n",
        "    return re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
        "\n",
        "# Fungsi untuk menghitung jumlah kata/token\n",
        "def count_tokens(text):\n",
        "    if pd.isnull(text) or text == '':\n",
        "        return 0\n",
        "    return len(word_tokenize(text))\n",
        "\n",
        "# 4-9. Preprocessing Lengkap\n",
        "def preprocessing(text):\n",
        "    if pd.isnull(text):\n",
        "        return ''\n",
        "\n",
        "    # 4. Casefolding\n",
        "    text = text.casefold()\n",
        "\n",
        "    # 5. Pembersihan Data\n",
        "    text = re.sub(r'@\\w+', '', text) # menghapus mention dalam data tweet\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text) # menghapus hashtag dalam data tweet\n",
        "    text = re.sub(r'http\\S+', '', text) # menghapus url/link pada data tweet\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text) #menghapus karakter selain huruf dan spasi\n",
        "    text = re.sub(r'\\d+', '', text) #menghapus semua angka\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() #menghapus spasi ganda pada tweet\n",
        "\n",
        "    # 6. Hilangkan elongation\n",
        "    text = remove_elongation(text)\n",
        "\n",
        "    # 7. Normalisasi slang\n",
        "    text = normalize_text(text)\n",
        "\n",
        "    # 8. Tokenisasi\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 9. Stopword removal\n",
        "    tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "    # 10. Stemming\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Hitung jumlah token sebelum preprocessing\n",
        "print(\"\\nMenghitung jumlah token sebelum preprocessing...\")\n",
        "df['token_count_before'] = df['full_text'].apply(count_tokens)\n",
        "total_tokens_before = df['token_count_before'].sum()\n",
        "\n",
        "# Proses preprocessing\n",
        "print(\"Melakukan preprocessing...\")\n",
        "df['cleaned_text'] = df['full_text'].apply(preprocessing)\n",
        "\n",
        "# Hitung jumlah token setelah preprocessing\n",
        "print(\"Menghitung jumlah token setelah preprocessing...\")\n",
        "df['token_count_after'] = df['cleaned_text'].apply(count_tokens)\n",
        "total_tokens_after = df['token_count_after'].sum()\n",
        "\n",
        "# Hapus baris dengan teks kosong setelah preprocessing\n",
        "df = df[df['cleaned_text'].str.strip() != ''].reset_index(drop=True)\n",
        "final_data_count = len(df)\n",
        "\n",
        "# Tampilkan statistik lengkap\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTIK DATA PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Jumlah data awal: {initial_data_count:,}\")\n",
        "print(f\"Jumlah data setelah filtering: {after_filter_count:,}\")\n",
        "print(f\"Jumlah data setelah menghapus duplikat: {after_dedup_count:,}\")\n",
        "print(f\"Jumlah data final (setelah preprocessing): {final_data_count:,}\")\n",
        "print(f\"Total data yang dibuang: {initial_data_count - final_data_count:,}\")\n",
        "print(f\"Persentase data yang tersisa: {(final_data_count/initial_data_count)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTIK TOKEN/KATA\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total token sebelum preprocessing: {total_tokens_before:,}\")\n",
        "print(f\"Total token setelah preprocessing: {total_tokens_after:,}\")\n",
        "print(f\"Token yang berkurang: {total_tokens_before - total_tokens_after:,}\")\n",
        "print(f\"Persentase token yang tersisa: {(total_tokens_after/total_tokens_before)*100:.2f}%\")\n",
        "print(f\"Rata-rata token per tweet sebelum: {total_tokens_before/final_data_count:.2f}\")\n",
        "print(f\"Rata-rata token per tweet setelah: {total_tokens_after/final_data_count:.2f}\")\n",
        "\n",
        "# Simpan hasil preprocessing\n",
        "output_filename = 'data-hasil-preprocessing-final.csv'\n",
        "df.to_csv(output_filename, index=False)\n",
        "files.download(output_filename)\n",
        "\n",
        "# Tampilkan contoh hasil\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONTOH HASIL PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "for i in range(min(3, len(df))):\n",
        "    print(f\"\\nContoh {i+1}:\")\n",
        "    print(f\"Teks Asli ({df['token_count_before'].iloc[i]} token):\")\n",
        "    print(f\"'{df['full_text'].iloc[i]}'\")\n",
        "    print(f\"Teks Bersih ({df['token_count_after'].iloc[i]} token):\")\n",
        "    print(f\"'{df['cleaned_text'].iloc[i]}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Tampilkan distribusi panjang token\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DISTRIBUSI PANJANG TOKEN\")\n",
        "print(\"=\"*60)\n",
        "print(\"Sebelum Preprocessing:\")\n",
        "print(f\"  Min token: {df['token_count_before'].min()}\")\n",
        "print(f\"  Max token: {df['token_count_before'].max()}\")\n",
        "print(f\"  Median token: {df['token_count_before'].median():.2f}\")\n",
        "\n",
        "print(\"\\nSetelah Preprocessing:\")\n",
        "print(f\"  Min token: {df['token_count_after'].min()}\")\n",
        "print(f\"  Max token: {df['token_count_after'].max()}\")\n",
        "print(f\"  Median token: {df['token_count_after'].median():.2f}\")\n",
        "\n",
        "print(\"\\nPreprocessing selesai! File telah disimpan dan diunduh.\")"
      ],
      "metadata": {
        "id": "hDftm5l8XPS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}