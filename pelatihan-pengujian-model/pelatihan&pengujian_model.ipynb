{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Pelatihan dan Pengujian Model**"
      ],
      "metadata": {
        "id": "LgA2TcY7YPPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install library yang diperlukan\n",
        "!pip install -U scikit-learn==1.6.1\n",
        "!pip install joblib\n",
        "!pip install scipy\n",
        "\n",
        "# Import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix, f1_score,\n",
        "    precision_score, recall_score, roc_auc_score, roc_curve, auc,\n",
        "    precision_recall_curve, average_precision_score, matthews_corrcoef,\n",
        "    cohen_kappa_score, hamming_loss, jaccard_score, log_loss,\n",
        "    balanced_accuracy_score, top_k_accuracy_score\n",
        ")\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from scipy.stats import loguniform, uniform\n",
        "from joblib import Memory\n",
        "import time\n",
        "import psutil\n",
        "import json\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup caching\n",
        "cachedir = './cache_dir'\n",
        "memory = Memory(location=cachedir, verbose=0)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('tweets-labeled.csv')\n",
        "\n",
        "# EDA Awal\n",
        "print(\"Distribusi Kelas:\")\n",
        "class_dist = df['label'].value_counts(normalize=True)\n",
        "print(class_dist)\n",
        "\n",
        "# Visualisasi distribusi kelas\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=class_dist.index, y=class_dist.values)\n",
        "plt.title('Distribusi Kelas Label')\n",
        "plt.ylabel('Proporsi')\n",
        "plt.xlabel('Kelas')\n",
        "plt.savefig('class_distribution.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Split data dengan stratifikasi\n",
        "X = df['cleaned_text']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nUkuran Dataset:\\nTrain: {X_train.shape[0]} sampel\\nTest: {X_test.shape[0]} sampel\")\n",
        "\n",
        "# Pipeline dengan One-vs-Rest (OvR)\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        sublinear_tf=True,\n",
        "    )),\n",
        "    ('ovr', OneVsRestClassifier(\n",
        "        LinearSVC(\n",
        "            class_weight='balanced',\n",
        "            dual=False,\n",
        "            random_state=42,\n",
        "            max_iter=2000\n",
        "        ),\n",
        "        n_jobs=-1\n",
        "    ))\n",
        "], memory=memory)\n",
        "\n",
        "# Parameter space yang dioptimalkan untuk OvR\n",
        "param_dist = {\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
        "    'tfidf__max_df': uniform(0.65, 0.3),\n",
        "    'tfidf__min_df': [0.0005, 0.001, 0.002, 0.005],\n",
        "    'tfidf__max_features': [8000, 12000, 15000, 20000],\n",
        "    'tfidf__use_idf': [True],\n",
        "    'ovr__estimator__C': loguniform(1e-2, 1e3),\n",
        "    'ovr__estimator__tol': [1e-4, 1e-3, 1e-2],\n",
        "    'ovr__estimator__loss': ['squared_hinge']\n",
        "}\n",
        "\n",
        "# Konfigurasi RandomizedSearch\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=100,\n",
        "    cv=skf,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=2,\n",
        "    refit=True,\n",
        "    error_score='raise',\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Monitoring sumber daya sebelum training\n",
        "start_time = time.time()\n",
        "mem_before = psutil.virtual_memory().used / (1024 ** 3)\n",
        "\n",
        "# Training dengan RandomizedSearchCV\n",
        "print(\"\\nMemulai proses tuning hyperparameter dengan One-vs-Rest...\")\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Monitoring sumber daya setelah training\n",
        "training_time = time.time() - start_time\n",
        "mem_after = psutil.virtual_memory().used / (1024 ** 3)\n",
        "print(f\"\\nWaktu training: {training_time/60:.2f} menit\")\n",
        "print(f\"Penggunaan memori: {mem_after - mem_before:.2f} GB\")\n",
        "\n",
        "# Analisis hasil tuning\n",
        "print(\"\\n=== Hasil Tuning (One-vs-Rest) ===\")\n",
        "print(\"Parameter Terbaik:\", search.best_params_)\n",
        "print(\"Skor F1 Terbaik (Validasi):\", search.best_score_)\n",
        "\n",
        "# Evaluasi pada test set\n",
        "best_model = search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.decision_function(X_test)\n",
        "\n",
        "# ==================== COMPREHENSIVE EVALUATION METRICS ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPREHENSIVE EVALUATION METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Basic Classification Metrics\n",
        "print(\"\\n=== 1. BASIC CLASSIFICATION METRICS ===\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
        "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
        "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
        "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
        "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
        "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
        "print(f\"Precision (Macro): {precision_macro:.4f}\")\n",
        "print(f\"Precision (Micro): {precision_micro:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision_weighted:.4f}\")\n",
        "print(f\"Recall (Macro): {recall_macro:.4f}\")\n",
        "print(f\"Recall (Micro): {recall_micro:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall_weighted:.4f}\")\n",
        "print(f\"F1-Score (Macro): {f1_macro:.4f}\")\n",
        "print(f\"F1-Score (Micro): {f1_micro:.4f}\")\n",
        "print(f\"F1-Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Top-k Accuracy (untuk k=2, k=3)\n",
        "if len(best_model.classes_) > 2:\n",
        "    for k in [2, 3]:\n",
        "        if k < len(best_model.classes_):\n",
        "            top_k_acc = top_k_accuracy_score(y_test, y_pred_proba, k=k, labels=best_model.classes_)\n",
        "            print(f\"Top-{k} Accuracy: {top_k_acc:.4f}\")\n",
        "\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\n=== 6. DETAILED CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# Per-Class Metrics Summary\n",
        "print(\"\\n=== 7. PER-CLASS METRICS SUMMARY ===\")\n",
        "per_class_precision = precision_score(y_test, y_pred, average=None)\n",
        "per_class_recall = recall_score(y_test, y_pred, average=None)\n",
        "per_class_f1 = f1_score(y_test, y_pred, average=None)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Class': best_model.classes_,\n",
        "    'Precision': per_class_precision,\n",
        "    'Recall': per_class_recall,\n",
        "    'F1-Score': per_class_f1,\n",
        "    'Support': [sum(y_test == cls) for cls in best_model.classes_]\n",
        "})\n",
        "print(metrics_df.round(4))\n",
        "\n",
        "# ==================== ADVANCED VISUALIZATIONS ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ADVANCED VISUALIZATIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Enhanced Confusion Matrix\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Normalized confusion matrix\n",
        "cm_norm = confusion_matrix(y_test, y_pred, normalize='true')\n",
        "sns.heatmap(cm_norm, annot=True, fmt=\".2%\", cmap='Blues',\n",
        "           xticklabels=best_model.classes_,\n",
        "           yticklabels=best_model.classes_, ax=axes[0,0])\n",
        "axes[0,0].set_title('Confusion Matrix (Normalized)')\n",
        "axes[0,0].set_xlabel('Predicted')\n",
        "axes[0,0].set_ylabel('True')\n",
        "\n",
        "# Absolute confusion matrix\n",
        "cm_abs = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm_abs, annot=True, fmt=\"d\", cmap='Blues',\n",
        "           xticklabels=best_model.classes_,\n",
        "           yticklabels=best_model.classes_, ax=axes[0,1])\n",
        "axes[0,1].set_title('Confusion Matrix (Absolute)')\n",
        "axes[0,1].set_xlabel('Predicted')\n",
        "axes[0,1].set_ylabel('True')\n",
        "\n",
        "# Per-class metrics bar plot\n",
        "metrics_plot = metrics_df.set_index('Class')[['Precision', 'Recall', 'F1-Score']]\n",
        "metrics_plot.plot(kind='bar', ax=axes[1,0])\n",
        "axes[1,0].set_title('Per-Class Metrics Comparison')\n",
        "axes[1,0].set_ylabel('Score')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Class distribution vs Performance\n",
        "class_counts = pd.Series(y_test).value_counts()\n",
        "performance_data = pd.DataFrame({\n",
        "    'Class': best_model.classes_,\n",
        "    'F1-Score': per_class_f1,\n",
        "    'Sample_Count': [class_counts[cls] for cls in best_model.classes_]\n",
        "})\n",
        "\n",
        "scatter = axes[1,1].scatter(performance_data['Sample_Count'],\n",
        "                          performance_data['F1-Score'],\n",
        "                          s=100, alpha=0.7, c=range(len(best_model.classes_)), cmap='viridis')\n",
        "axes[1,1].set_xlabel('Number of Test Samples')\n",
        "axes[1,1].set_ylabel('F1-Score')\n",
        "axes[1,1].set_title('F1-Score vs Sample Count')\n",
        "for i, txt in enumerate(performance_data['Class']):\n",
        "    axes[1,1].annotate(txt, (performance_data['Sample_Count'].iloc[i],\n",
        "                            performance_data['F1-Score'].iloc[i]),\n",
        "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comprehensive_evaluation_plots.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Decision Score Distribution Analysis\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Decision scores distribution for each class\n",
        "n_classes = len(best_model.classes_)\n",
        "fig, axes = plt.subplots(2, (n_classes + 1) // 2, figsize=(15, 8))\n",
        "axes = axes.flatten() if n_classes > 2 else [axes]\n",
        "\n",
        "for i, class_name in enumerate(best_model.classes_):\n",
        "    if i < len(axes):\n",
        "        # Get decision scores for this class\n",
        "        class_scores = y_pred_proba[:, i]\n",
        "\n",
        "        # Separate scores by true label\n",
        "        true_class_scores = class_scores[y_test == class_name]\n",
        "        other_class_scores = class_scores[y_test != class_name]\n",
        "\n",
        "        axes[i].hist(other_class_scores, bins=30, alpha=0.7, label=f'Other Classes', color='red')\n",
        "        axes[i].hist(true_class_scores, bins=30, alpha=0.7, label=f'True {class_name}', color='blue')\n",
        "        axes[i].set_title(f'Decision Scores: {class_name}')\n",
        "        axes[i].set_xlabel('Decision Score')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(n_classes, len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('decision_scores_distribution.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ==================== MODEL ANALYSIS ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Feature Importance Analysis\n",
        "if hasattr(best_model.named_steps['ovr'], 'estimators_'):\n",
        "    feature_names = best_model.named_steps['tfidf'].get_feature_names_out()\n",
        "\n",
        "    print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
        "\n",
        "    # Create comprehensive feature importance analysis\n",
        "    feature_importance_data = []\n",
        "\n",
        "    for i, (class_label, estimator) in enumerate(zip(best_model.classes_, best_model.named_steps['ovr'].estimators_)):\n",
        "        if hasattr(estimator, 'coef_'):\n",
        "            coef = estimator.coef_[0]\n",
        "\n",
        "            # Top positive and negative features\n",
        "            top_positive_indices = np.argsort(coef)[-10:][::-1]\n",
        "            top_negative_indices = np.argsort(coef)[:10]\n",
        "\n",
        "            for idx in top_positive_indices:\n",
        "                feature_importance_data.append({\n",
        "                    'Class': class_label,\n",
        "                    'Feature': feature_names[idx],\n",
        "                    'Weight': coef[idx],\n",
        "                    'Type': 'Positive'\n",
        "                })\n",
        "\n",
        "            for idx in top_negative_indices:\n",
        "                feature_importance_data.append({\n",
        "                    'Class': class_label,\n",
        "                    'Feature': feature_names[idx],\n",
        "                    'Weight': coef[idx],\n",
        "                    'Type': 'Negative'\n",
        "                })\n",
        "\n",
        "    # Convert to DataFrame for analysis\n",
        "    feature_df = pd.DataFrame(feature_importance_data)\n",
        "\n",
        "    # Plot feature importance\n",
        "    fig, axes = plt.subplots(len(best_model.classes_), 1, figsize=(12, 4*len(best_model.classes_)))\n",
        "    if len(best_model.classes_) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, class_name in enumerate(best_model.classes_):\n",
        "        class_features = feature_df[feature_df['Class'] == class_name]\n",
        "        class_features_sorted = class_features.reindex(class_features['Weight'].abs().sort_values(ascending=True).index)\n",
        "\n",
        "        colors = ['red' if x < 0 else 'blue' for x in class_features_sorted['Weight']]\n",
        "        axes[i].barh(range(len(class_features_sorted)), class_features_sorted['Weight'], color=colors, alpha=0.7)\n",
        "        axes[i].set_yticks(range(len(class_features_sorted)))\n",
        "        axes[i].set_yticklabels(class_features_sorted['Feature'], fontsize=8)\n",
        "        axes[i].set_title(f'Feature Importance: {class_name}')\n",
        "        axes[i].set_xlabel('Weight')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "        axes[i].axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance_analysis.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "# Cross-Validation Analysis\n",
        "print(\"\\n=== CROSS-VALIDATION ANALYSIS ===\")\n",
        "cv_results = pd.DataFrame(search.cv_results_)\n",
        "cv_results_sorted = cv_results.sort_values(by='rank_test_score').head(10)\n",
        "\n",
        "print(\"Top 10 Parameter Combinations:\")\n",
        "for i, (idx, row) in enumerate(cv_results_sorted.iterrows()):\n",
        "    print(f\"\\nRank {i+1}:\")\n",
        "    print(f\"  Mean CV Score: {row['mean_test_score']:.4f} (+/- {row['std_test_score']*2:.4f})\")\n",
        "    print(f\"  Parameters: {row['params']}\")\n",
        "\n",
        "# CV scores visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.scatter(cv_results['mean_train_score'], cv_results['mean_test_score'],\n",
        "           alpha=0.6, c=cv_results['param_ovr__estimator__C'], cmap='viridis')\n",
        "plt.colorbar(label='C parameter')\n",
        "plt.xlabel('Mean Train Score')\n",
        "plt.ylabel('Mean CV Score')\n",
        "plt.title('Train vs CV Score')\n",
        "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(cv_results['mean_test_score'], cv_results['std_test_score'], alpha=0.6)\n",
        "plt.xlabel('Mean CV Score')\n",
        "plt.ylabel('Std CV Score')\n",
        "plt.title('CV Score vs Stability')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.hist(cv_results['mean_test_score'], bins=20, alpha=0.7)\n",
        "plt.xlabel('Mean CV Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of CV Scores')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "top_params = cv_results_sorted.head(20)\n",
        "plt.scatter(top_params['param_ovr__estimator__C'], top_params['mean_test_score'], alpha=0.7)\n",
        "plt.xscale('log')\n",
        "plt.xlabel('C Parameter (log scale)')\n",
        "plt.ylabel('Mean CV Score')\n",
        "plt.title('C Parameter vs Performance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cross_validation_analysis.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# ==================== COMPREHENSIVE SUMMARY ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPREHENSIVE MODEL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create summary dictionary\n",
        "summary = {\n",
        "    'Model_Type': 'One-vs-Rest Linear SVM',\n",
        "    'Dataset_Size': {\n",
        "        'Train': len(X_train),\n",
        "        'Test': len(X_test),\n",
        "        'Features': len(feature_names) if 'feature_names' in locals() else 'N/A'\n",
        "    },\n",
        "    'Classes': {\n",
        "        'Count': len(best_model.classes_),\n",
        "        'Names': list(best_model.classes_)\n",
        "    },\n",
        "    'Performance_Metrics': {\n",
        "        'Accuracy': round(accuracy, 4),\n",
        "        'Balanced_Accuracy': round(balanced_acc, 4),\n",
        "        'F1_Macro': round(f1_macro, 4),\n",
        "        'F1_Micro': round(f1_micro, 4),\n",
        "        'F1_Weighted': round(f1_weighted, 4),\n",
        "        'Precision_Macro': round(precision_macro, 4),\n",
        "        'Recall_Macro': round(recall_macro, 4),\n",
        "    },\n",
        "    'Best_Parameters': search.best_params_,\n",
        "    'Cross_Validation': {\n",
        "        'Best_Score': round(search.best_score_, 4),\n",
        "        'Std_Score': round(cv_results.loc[cv_results['rank_test_score'] == 1, 'std_test_score'].iloc[0], 4)\n",
        "    },\n",
        "    'Training_Info': {\n",
        "        'Training_Time_Minutes': round(training_time/60, 2),\n",
        "        'Memory_Usage_GB': round(mem_after - mem_before, 2)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Print summary\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "# Save summary to file\n",
        "with open('model_evaluation_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# Save detailed metrics to CSV\n",
        "detailed_metrics = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Balanced_Accuracy', 'Precision_Macro', 'Precision_Micro', 'Precision_Weighted',\n",
        "               'Recall_Macro', 'Recall_Micro', 'Recall_Weighted', 'F1_Macro', 'F1_Micro', 'F1_Weighted'\n",
        "               ],\n",
        "    'Score': [accuracy, balanced_acc, precision_macro, precision_micro, precision_weighted,\n",
        "              recall_macro, recall_micro, recall_weighted, f1_macro, f1_micro, f1_weighted,\n",
        "              ]\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "if 'avg_precision_macro' in locals():\n",
        "    detailed_metrics = pd.concat([detailed_metrics, pd.DataFrame({\n",
        "        'Metric': ['Avg_Precision_Macro', 'Avg_Precision_Micro', 'Avg_Precision_Weighted'],\n",
        "    })], ignore_index=True)\n",
        "\n",
        "detailed_metrics.to_csv('detailed_evaluation_metrics.csv', index=False)\n",
        "\n",
        "# Save per-class metrics\n",
        "metrics_df.to_csv('per_class_metrics.csv', index=False)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_model, 'optimized_svm_ovr_model.pkl')\n",
        "\n",
        "print(f\"\\n=== FILES SAVED ===\")\n",
        "print(\"1. optimized_svm_ovr_model.pkl - Trained model\")\n",
        "print(\"2. model_evaluation_summary.json - Comprehensive summary\")\n",
        "print(\"3. detailed_evaluation_metrics.csv - All metrics\")\n",
        "print(\"4. per_class_metrics.csv - Per-class performance\")\n",
        "print(\"5. Multiple visualization PNG files\")\n",
        "\n",
        "# Clean up\n",
        "memory.clear(warn=False)\n",
        "\n",
        "# ==================== ADDITIONAL ADVANCED ANALYSIS ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ADDITIONAL ADVANCED ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Learning Curve Analysis (if computational resources allow)\n",
        "print(\"\\n=== LEARNING CURVE ANALYSIS ===\")\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "try:\n",
        "    # Use a subset for learning curve to save computation time\n",
        "    sample_sizes = np.linspace(0.1, 1.0, 10)\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        best_model, X_train, y_train,\n",
        "        train_sizes=sample_sizes,\n",
        "        cv=3, scoring='f1_weighted', n_jobs=-1, random_state=42\n",
        "    )\n",
        "\n",
        "    # Calculate mean and std\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score')\n",
        "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
        "    plt.xlabel('Training Set Size')\n",
        "    plt.ylabel('F1 Score (Weighted)')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('learning_curve.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Final training score: {train_mean[-1]:.4f} (+/- {train_std[-1]:.4f})\")\n",
        "    print(f\"Final validation score: {val_mean[-1]:.4f} (+/- {val_std[-1]:.4f})\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Learning curve analysis failed: {e}\")\n",
        "\n",
        "# Validation Curve for Key Hyperparameters\n",
        "print(\"\\n=== VALIDATION CURVE ANALYSIS ===\")\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "try:\n",
        "    # Validation curve for C parameter\n",
        "    param_range = np.logspace(-2, 2, 10)\n",
        "    train_scores, val_scores = validation_curve(\n",
        "        Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(**{k.replace('tfidf__', ''): v for k, v in search.best_params_.items() if k.startswith('tfidf__')})),\n",
        "            ('ovr', OneVsRestClassifier(LinearSVC(class_weight='balanced', dual=False, random_state=42), n_jobs=-1))\n",
        "        ]),\n",
        "        X_train, y_train, param_name='ovr__estimator__C', param_range=param_range,\n",
        "        cv=3, scoring='f1_weighted', n_jobs=-1\n",
        "    )\n",
        "\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.semilogx(param_range, train_mean, 'o-', color='blue', label='Training Score')\n",
        "    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "    plt.semilogx(param_range, val_mean, 'o-', color='red', label='Validation Score')\n",
        "    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
        "    plt.xlabel('C Parameter')\n",
        "    plt.ylabel('F1 Score (Weighted)')\n",
        "    plt.title('Validation Curve for C Parameter')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('validation_curve_C.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Validation curve analysis failed: {e}\")\n",
        "\n",
        "# 3. Error Analysis\n",
        "print(\"\\n=== ERROR ANALYSIS ===\")\n",
        "\n",
        "# Get misclassified samples\n",
        "misclassified_mask = y_test != y_pred\n",
        "misclassified_indices = np.where(misclassified_mask)[0]\n",
        "\n",
        "print(f\"Total misclassified samples: {len(misclassified_indices)}\")\n",
        "print(f\"Misclassification rate: {len(misclassified_indices)/len(y_test)*100:.2f}%\")\n",
        "\n",
        "# Analyze misclassification patterns\n",
        "misclass_df = pd.DataFrame({\n",
        "    'True_Label': y_test.iloc[misclassified_indices],\n",
        "    'Predicted_Label': y_pred[misclassified_indices],\n",
        "    'Text': X_test.iloc[misclassified_indices]\n",
        "})\n",
        "\n",
        "# Most common misclassification patterns\n",
        "print(\"\\nMost Common Misclassification Patterns:\")\n",
        "misclass_patterns = misclass_df.groupby(['True_Label', 'Predicted_Label']).size().sort_values(ascending=False)\n",
        "print(misclass_patterns.head(10))\n",
        "\n",
        "# Misclassification matrix (confusion matrix for errors only)\n",
        "plt.figure(figsize=(10, 8))\n",
        "misclass_matrix = pd.crosstab(misclass_df['True_Label'], misclass_df['Predicted_Label'])\n",
        "sns.heatmap(misclass_matrix, annot=True, fmt='d', cmap='Reds')\n",
        "plt.title('Misclassification Matrix (Error Patterns)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.savefig('misclassification_matrix.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Confidence Analysis\n",
        "print(\"\\n=== CONFIDENCE ANALYSIS ===\")\n",
        "\n",
        "# Calculate confidence scores (max decision score for each prediction)\n",
        "confidence_scores = np.max(y_pred_proba, axis=1)\n",
        "correct_predictions = y_test == y_pred\n",
        "\n",
        "# Analyze confidence distribution\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.hist(confidence_scores[correct_predictions], bins=30, alpha=0.7, label='Correct', color='green')\n",
        "plt.hist(confidence_scores[~correct_predictions], bins=30, alpha=0.7, label='Incorrect', color='red')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Confidence Distribution: Correct vs Incorrect')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "confidence_bins = np.linspace(confidence_scores.min(), confidence_scores.max(), 10)\n",
        "bin_accuracy = []\n",
        "bin_centers = []\n",
        "for i in range(len(confidence_bins)-1):\n",
        "    mask = (confidence_scores >= confidence_bins[i]) & (confidence_scores < confidence_bins[i+1])\n",
        "    if np.sum(mask) > 0:\n",
        "        bin_accuracy.append(np.mean(correct_predictions[mask]))\n",
        "        bin_centers.append((confidence_bins[i] + confidence_bins[i+1]) / 2)\n",
        "\n",
        "plt.plot(bin_centers, bin_accuracy, 'o-')\n",
        "plt.plot([confidence_scores.min(), confidence_scores.max()],\n",
        "         [confidence_scores.min(), confidence_scores.max()], 'k--', alpha=0.5)\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Reliability Diagram')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "for class_name in best_model.classes_:\n",
        "    class_mask = y_test == class_name\n",
        "    if np.sum(class_mask) > 0:\n",
        "        plt.hist(confidence_scores[class_mask], bins=20, alpha=0.5, label=class_name)\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Confidence by True Class')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.scatter(confidence_scores, correct_predictions.astype(int), alpha=0.6)\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Correct (1) / Incorrect (0)')\n",
        "plt.title('Confidence vs Correctness')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confidence_analysis.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Feature Analysis Deep Dive\n",
        "print(\"\\n=== FEATURE ANALYSIS DEEP DIVE ===\")\n",
        "\n",
        "if hasattr(best_model.named_steps['ovr'], 'estimators_'):\n",
        "    # Analyze feature usage across all classifiers\n",
        "    all_features = best_model.named_steps['tfidf'].get_feature_names_out()\n",
        "    feature_usage = np.zeros(len(all_features))\n",
        "\n",
        "    for estimator in best_model.named_steps['ovr'].estimators_:\n",
        "        if hasattr(estimator, 'coef_'):\n",
        "            feature_usage += np.abs(estimator.coef_[0])\n",
        "\n",
        "    # Most important features overall\n",
        "    top_feature_indices = np.argsort(feature_usage)[-50:][::-1]\n",
        "\n",
        "    print(\"Top 50 Most Important Features Across All Classes:\")\n",
        "    for i, idx in enumerate(top_feature_indices):\n",
        "        print(f\"{i+1:2d}. {all_features[idx]}: {feature_usage[idx]:.4f}\")\n",
        "\n",
        "    # Feature importance visualization\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    top_features = all_features[top_feature_indices[:20]]\n",
        "    top_weights = feature_usage[top_feature_indices[:20]]\n",
        "\n",
        "    plt.barh(range(len(top_features)), top_weights)\n",
        "    plt.yticks(range(len(top_features)), top_features)\n",
        "    plt.xlabel('Cumulative Absolute Weight')\n",
        "    plt.title('Top 20 Most Important Features (Cumulative)')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('top_features_cumulative.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Performance Stability Analysis\n",
        "print(\"\\n=== PERFORMANCE STABILITY ANALYSIS ===\")\n",
        "\n",
        "# Analyze CV scores distribution\n",
        "cv_scores = []\n",
        "for fold in range(skf.n_splits):\n",
        "    fold_col = f'split{fold}_test_score'\n",
        "    if fold_col in cv_results.columns:\n",
        "        cv_scores.extend(cv_results[fold_col].values)\n",
        "\n",
        "if cv_scores:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(cv_scores, bins=30, alpha=0.7)\n",
        "    plt.xlabel('CV Score')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of All CV Scores')\n",
        "    plt.axvline(np.mean(cv_scores), color='red', linestyle='--', label=f'Mean: {np.mean(cv_scores):.4f}')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.boxplot([cv_results[f'split{fold}_test_score'].values for fold in range(skf.n_splits)\n",
        "                 if f'split{fold}_test_score' in cv_results.columns])\n",
        "    plt.xlabel('CV Fold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Score Distribution by CV Fold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('performance_stability.png', bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"CV Score Statistics:\")\n",
        "    print(f\"  Mean: {np.mean(cv_scores):.4f}\")\n",
        "    print(f\"  Std: {np.std(cv_scores):.4f}\")\n",
        "    print(f\"  Min: {np.min(cv_scores):.4f}\")\n",
        "    print(f\"  Max: {np.max(cv_scores):.4f}\")\n",
        "\n",
        "# Model Complexity Analysis\n",
        "print(\"\\n=== MODEL COMPLEXITY ANALYSIS ===\")\n",
        "\n",
        "# Analyze sparsity of the model\n",
        "total_features = len(all_features)\n",
        "non_zero_features = 0\n",
        "total_coefficients = 0\n",
        "\n",
        "for estimator in best_model.named_steps['ovr'].estimators_:\n",
        "    if hasattr(estimator, 'coef_'):\n",
        "        coef = estimator.coef_[0]\n",
        "        non_zero_features += np.sum(coef != 0)\n",
        "        total_coefficients += len(coef)\n",
        "\n",
        "sparsity = 1 - (non_zero_features / total_coefficients)\n",
        "print(f\"Model Sparsity: {sparsity:.4f} ({non_zero_features}/{total_coefficients} non-zero coefficients)\")\n",
        "print(f\"Average non-zero features per classifier: {non_zero_features/len(best_model.named_steps['ovr'].estimators_):.0f}\")\n",
        "\n",
        "\n",
        "# ==================== CROSS-VALIDATION FOLD PERFORMANCE ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CROSS-VALIDATION FOLD PERFORMANCE (BEST MODEL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ambil indeks model terbaik\n",
        "best_index = search.best_index_\n",
        "\n",
        "# Ambil skor untuk setiap fold\n",
        "fold_scores = []\n",
        "for i in range(skf.n_splits):\n",
        "    fold_score = search.cv_results_[f'split{i}_test_score'][best_index]\n",
        "    fold_scores.append(fold_score)\n",
        "    print(f\"Fold {i+1} F1 Weighted: {fold_score:.4f}\")\n",
        "\n",
        "# Hitung statistik\n",
        "mean_score = np.mean(fold_scores)\n",
        "std_score = np.std(fold_scores)\n",
        "min_score = np.min(fold_scores)\n",
        "max_score = np.max(fold_scores)\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(f\"Rata-rata F1: {mean_score:.4f}\")\n",
        "print(f\"Standar Deviasi: {std_score:.4f}\")\n",
        "print(f\"Min F1: {min_score:.4f}\")\n",
        "print(f\"Max F1: {max_score:.4f}\")\n",
        "\n",
        "# Visualisasi kinerja per fold\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, skf.n_splits+1), fold_scores, 'o-', label='F1 per Fold')\n",
        "plt.axhline(y=mean_score, color='r', linestyle='--', label=f'Rata-rata ({mean_score:.4f})')\n",
        "plt.fill_between(range(1, skf.n_splits+1),\n",
        "                 mean_score - std_score,\n",
        "                 mean_score + std_score,\n",
        "                 alpha=0.1, color='g', label='±1 SD')\n",
        "\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('F1 Score (Weighted)')\n",
        "plt.title('Kinerja Model Terbaik pada Setiap Fold Cross-Validation')\n",
        "plt.xticks(range(1, skf.n_splits+1))\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig('best_model_cv_performance.png', bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Tambahkan informasi ke summary\n",
        "summary['Cross_Validation']['Fold_Scores'] = fold_scores\n",
        "summary['Cross_Validation']['Mean'] = round(mean_score, 4)\n",
        "summary['Cross_Validation']['Std'] = round(std_score, 4)\n",
        "summary['Cross_Validation']['Min'] = round(min_score, 4)\n",
        "summary['Cross_Validation']['Max'] = round(max_score, 4)\n",
        "\n",
        "# Final Comprehensive Report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL COMPREHENSIVE EVALUATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "final_report = f\"\"\"\n",
        "MODEL PERFORMANCE SUMMARY\n",
        "========================\n",
        "✓ Model Type: One-vs-Rest Linear SVM\n",
        "✓ Classes: {len(best_model.classes_)} ({', '.join(best_model.classes_)})\n",
        "✓ Training Samples: {len(X_train):,}\n",
        "✓ Test Samples: {len(X_test):,}\n",
        "✓ Features: {total_features:,}\n",
        "\n",
        "PERFORMANCE METRICS\n",
        "==================\n",
        "✓ Accuracy: {accuracy:.4f}\n",
        "✓ Balanced Accuracy: {balanced_acc:.4f}\n",
        "✓ F1-Score (Macro): {f1_macro:.4f}\n",
        "✓ F1-Score (Weighted): {f1_weighted:.4f}\n",
        "✓ Precision (Macro): {precision_macro:.4f}\n",
        "✓ Recall (Macro): {recall_macro:.4f}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "final_report += f\"\"\"\n",
        "MODEL CHARACTERISTICS\n",
        "====================\n",
        "✓ Model Sparsity: {sparsity:.4f}\n",
        "✓ Training Time: {training_time/60:.2f} minutes\n",
        "✓ Memory Usage: {mem_after - mem_before:.2f} GB\n",
        "✓ Cross-Validation Score: {search.best_score_:.4f}\n",
        "✓ Hyperparameter Combinations Tested: {len(cv_results)}\n",
        "\n",
        "BEST HYPERPARAMETERS\n",
        "===================\n",
        "\"\"\"\n",
        "\n",
        "for param, value in search.best_params_.items():\n",
        "    final_report += f\"✓ {param}: {value}\\n\"\n",
        "\n",
        "final_report += f\"\"\"\n",
        "ERROR ANALYSIS\n",
        "=============\n",
        "✓ Misclassified Samples: {len(misclassified_indices)} ({len(misclassified_indices)/len(y_test)*100:.2f}%)\n",
        "✓ Most Problematic Class Pairs: {misclass_patterns.head(3).to_dict()}\n",
        "\n",
        "RECOMMENDATIONS\n",
        "==============\n",
        "\"\"\"\n",
        "\n",
        "if accuracy > 0.8:\n",
        "    final_report += \"✓ Model shows GOOD performance\\n\"\n",
        "elif accuracy > 0.7:\n",
        "    final_report += \"✓ Model shows ACCEPTABLE performance\\n\"\n",
        "else:\n",
        "    final_report += \"⚠ Model shows POOR performance - consider feature engineering or different algorithms\\n\"\n",
        "\n",
        "if sparsity > 0.8:\n",
        "    final_report += \"✓ Model is highly sparse - good for interpretability\\n\"\n",
        "elif sparsity > 0.5:\n",
        "    final_report += \"✓ Model has moderate sparsity\\n\"\n",
        "elif sparsity > 0.2:\n",
        "    final_report += \"⚠ Model uses most features - consider feature selection\\n\"\n",
        "\n",
        "else:\n",
        "    final_report += \"⚠ Weak correlation - model may not be reliable\\n\"\n",
        "\n",
        "print(final_report)\n",
        "\n",
        "# Save final report\n",
        "with open('final_evaluation_report.txt', 'w') as f:\n",
        "    f.write(final_report)\n",
        "\n",
        "print(\"\\n=== ALL ANALYSIS COMPLETE ===\")\n",
        "print(\"Generated files:\")\n",
        "print(\"1. optimized_svm_ovr_model.pkl - Trained model\")\n",
        "print(\"2. model_evaluation_summary.json - JSON summary\")\n",
        "print(\"3. detailed_evaluation_metrics.csv - All metrics\")\n",
        "print(\"4. per_class_metrics.csv - Per-class performance\")\n",
        "print(\"5. final_evaluation_report.txt - Human-readable report\")\n",
        "print(\"6. Multiple visualization PNG files\")\n",
        "print(\"\\nCache cleared successfully!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "xtHMDa3KYSmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}